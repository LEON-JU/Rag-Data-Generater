import json
from transformers import AutoTokenizer
from tqdm import tqdm

# è®¾ç½®è·¯å¾„
input_path = "/home/juyiang/data/dataset/sft_data/multi_agent/twowikimultihopqa/summary/train.jsonl"
output_path = "/home/juyiang/data/dataset/sft_data/multi_agent/twowikimultihopqa/summary/train_filtered.jsonl"

# Token é™åˆ¶
MAX_TOKENS = 8192

# åŠ è½½æœ¬åœ° tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    "/home/juyiang/data/llm_models/deepseek-r1-0528-qwen3-8b",
    trust_remote_code=True,
    local_files_only=True
)

# è®¡æ•°
total = 0
kept = 0
dropped = 0

with open(input_path, "r", encoding="utf-8") as fin, open(output_path, "w", encoding="utf-8") as fout:
    for line in tqdm(fin, desc="Filtering samples"):
        total += 1
        try:
            data = json.loads(line)
            messages = data.get("messages", [])
            full_text = "\n".join([m["content"] for m in messages if "content" in m])

            tokens = tokenizer.encode(full_text, add_special_tokens=False)

            if len(tokens) <= MAX_TOKENS:
                fout.write(json.dumps(data, ensure_ascii=False) + "\n")
                kept += 1
            else:
                dropped += 1
        except Exception as e:
            print(f"[Error] Skipping line {total}: {e}")
            dropped += 1

# ç»“æœç»Ÿè®¡
print("\nâœ… å¤„ç†å®Œæˆ")
print(f"ğŸ“¦ æ€»æ ·æœ¬æ•°: {total}")
print(f"âœ… ä¿ç•™æ ·æœ¬: {kept}")
print(f"ğŸ—‘ï¸ ä¸¢å¼ƒæ ·æœ¬: {dropped}")
print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {output_path}")
