import json
from pathlib import Path
from statistics import mean, median
from transformers import AutoTokenizer
from tqdm import tqdm

# è®¾ç½®æœ¬åœ° tokenizer è·¯å¾„
tokenizer = AutoTokenizer.from_pretrained(
    "/home/juyiang/data/llm_models/deepseek-r1-0528-qwen3-8b-AddTags2",
    trust_remote_code=True
)

# JSONL æ•°æ®è·¯å¾„
file_path = "/home/juyiang/data/dataset/sft_data/multi_agent/hotpotqa_fullwiki/summary/train.jsonl"

# å­˜å‚¨æ¯æ¡æ•°æ®çš„ token é•¿åº¦
token_lens = []

with open(file_path, "r", encoding="utf-8") as f:
    total_lines = sum(1 for _ in f)

# è¯»å–å¹¶å¤„ç†æ•°æ®
with open(file_path, "r", encoding="utf-8") as f:
    for line in tqdm(f, total=total_lines, desc="Processing samples"):
        data = json.loads(line)
        messages = data.get("messages", [])

        # æ‹¼æ¥æ‰€æœ‰ message çš„ content
        full_text = "\n".join([m["content"] for m in messages if "content" in m])

        # ä½¿ç”¨ tokenizer ç¼–ç å¹¶ç»Ÿè®¡ token æ•°é‡
        tokens = tokenizer.encode(full_text, add_special_tokens=False)
        token_lens.append(len(tokens))

# æ‰“å°ç»Ÿè®¡ç»“æœ
print(f"ğŸ“Š æ ·æœ¬æ€»æ•°: {len(token_lens)}")
print(f"ğŸ”¢ æœ€å¤§é•¿åº¦ (max): {max(token_lens)}")
print(f"ğŸ” æœ€å°é•¿åº¦ (min): {min(token_lens)}")
print(f"ğŸ“ˆ å¹³å‡é•¿åº¦ (mean): {mean(token_lens):.2f}")
print(f"ğŸ“‰ ä¸­ä½æ•° (median): {median(token_lens)}")
